{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#텍스트 import , encoding\n",
    "#text = upload_files(\"input_text1.txt\")\n",
    "\n",
    "\n",
    "def upload_files(file_name):\n",
    "    with open(file_name) as file_object:\n",
    "        texts = file_object.read()\n",
    "        #texts = open(file_name,\"r\", encoding='euc-kr')\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(texts):\n",
    "    #초기 실행시 아래 pip 다 실행해야함!\n",
    "    #!pip install git+https://github.com/ssut/py-hanspell.git\n",
    "    #!pip install konlpy\n",
    "    #!pip install krwordrank\n",
    "    #오류 발생시 : https://data-scientist-brian-kim.tistory.com/79 침고\n",
    "    \n",
    "    from hanspell import spell_checker\n",
    "    from tqdm.notebook import tqdm\n",
    "    from konlpy.tag import Twitter\n",
    "    from collections import Counter\n",
    "    from krwordrank.hangle import normalize\n",
    "\n",
    "\n",
    "    nlpy = Twitter()\n",
    "    \n",
    "    lines = [line.rstrip('\\n') for line in texts]\n",
    "    nouns_word = [] #명사 단어 추출\n",
    "    normalized_lines = []\n",
    "    for each_line in tqdm(lines):\n",
    "        each_line = each_line.replace(\"\\x0c\", \"\") #json을 로드 하면서 생기는 특수문자 제거\n",
    "        each_line = normalize(each_line, english=True, number=True) #특수문자 제거\n",
    "        each_line = spell_checker.check(each_line).checked #맞춤법 틀린게 있다면 고쳐줌\n",
    "        nouns_word = nouns_word + nlpy.nouns(each_line) # 명사 단어 추출\n",
    "        normalized_lines.append(each_line)\n",
    "    \n",
    "    return lines, nouns_word, normalized_lines\n",
    "\n",
    "#lines : text를 단순히 \\n 기준으로 split 한 것\n",
    "#nouns_word : 각 문장에서 명사 단어\n",
    "#normaliezed_lines : 맞춤범 검사, 특수 기호 제거 된 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#추출된 명사에서 빈도수 세기\n",
    "def count_noun(nouns_word):\n",
    "    import pandas as pd\n",
    "    from collections import Counter\n",
    "    count = Counter(nouns_word)\n",
    "\n",
    "    tag_count = []\n",
    "    tags = []\n",
    "\n",
    "    #길이가 2부터 49까지인 단어 \n",
    "    for n, c in count.most_common():\n",
    "      dics = {'tag': n, 'count': c}\n",
    "      if len(dics['tag']) >= 2 and len(tags) <= 49:\n",
    "        tag_count.append(dics)\n",
    "        tags.append(dics['tag'])\n",
    "        \n",
    "    noun_count_df = pd.DataFrame(tag_count)\n",
    "    \n",
    "    return noun_count_df\n",
    "\n",
    "#컬럼이 tag와 count로 된 dataframe 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한글 단어의 중요도 순위 측정\n",
    "def extract_krwordrank(normalized_lines, noun_count_df):\n",
    "    from krwordrank.word import KRWordRank\n",
    "\n",
    "    wordrank_extractor = KRWordRank(\n",
    "        min_count = min(noun_count_df[\"count\"]), # 단어의 최소 출현 빈도수\n",
    "        max_length = max(noun_count_df[\"tag\"].str.len()), # 단어의 최대 길이\n",
    "        verbose = True\n",
    "        )\n",
    "\n",
    "    beta = 0.85    # PageRank의 decaying factor beta\n",
    "    max_iter = 10\n",
    "\n",
    "    #keywords는 filtering이 적용된 L parts\n",
    "    #rank는 substriing graph의 substring에 대한 구마\n",
    "    #graph는 substring graph\n",
    "    keywords, rank, graph = wordrank_extractor.extract(normalized_lines, beta, max_iter)\n",
    "    \n",
    "    keyword_df = pd.DataFrame(list(keywords.items()),columns=['word', 'rank'])\n",
    "    \n",
    "    return keyword_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wordlists(noun_count_df,keyword_df):\n",
    "    #[\"count\"]>0 수를 조정하여 빈칸 채울 단어 갯수나 난이도를 조정할 수 있음\n",
    "    frequency_noun_list = noun_count_df[noun_count_df[\"count\"]>0][\"tag\"].tolist()\n",
    "    keyword_list = keyword_df[\"word\"].tolist()\n",
    "    return frequency_noun_list,keyword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency_noun_list 기준으로 빈칸을 생성했을 때\n",
    "\n",
    "def __test_by_frequency_noun__(frequency_noun_list):\n",
    "    test_by_frequency_noun = []\n",
    "    for each_line in lines:\n",
    "        for word in each_line.split():\n",
    "            if word in frequency_noun_list:\n",
    "                each_line = each_line.replace(word, \"□\"*len(word))\n",
    "        test_by_frequency_noun.append(each_line)\n",
    "        \n",
    "    return test_by_frequency_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword_list 기준으로 빈칸을 생성했을 때\n",
    "def __test_by_keyword_list__(keyword_list):\n",
    "    test_by_keyword_list = []\n",
    "    for each_line in lines:\n",
    "        for word in each_line.split():\n",
    "            if word in keyword_list:\n",
    "                each_line = each_line.replace(word, \"□\"*len(word))\n",
    "        test_by_keyword.append(each_line)\n",
    "    \n",
    "    return test_by_keyword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 문장에 3개 이상 빈칸이 오지 않도록 갯수 조정\n",
    "\n",
    "def __test_limit_count__(lines, frequency_noun_list,keyword_list):\n",
    "    test_limit_count = []\n",
    "    for each_line in lines:\n",
    "        replace_word = []\n",
    "        for word in each_line.split(' '):\n",
    "            if word in keyword_list:\n",
    "                replace_word.append(word)\n",
    "\n",
    "        #한 문장에 빈칸이 3개 이상일때\n",
    "        if len(replace_word) > 2:\n",
    "            A = pd.DataFrame()\n",
    "            pattern = '|'.join(replace_word)\n",
    "            A = keyword_df[keyword_df['word'].str.contains(pattern, case=False)]\n",
    "            replace_word = A[:3].word\n",
    "\n",
    "        for i in replace_word:\n",
    "            each_line = each_line.replace(i,\"□\"*len(i))\n",
    "\n",
    "        test_limit_count.append(each_line)\n",
    "        \n",
    "    return test_limit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __main__():\n",
    "    #upload_files 이 부분은 서향님이 코드 실행시킨거 결과물 넣으면 돼서 필요 없을거 같아요\n",
    "    upload_files(\"\")\n",
    "    lines, nouns_word, normalized_lines = preprocessing_text(texts)\n",
    "    noun_count_df = count_noun(nouns_word)\n",
    "    keyword_df = extract_krwordrank(normalized_lines, noun_count_df)\n",
    "    frequency_noun_list,keyword_list = make_wordlists(noun_count_df,keyword_df)\n",
    "    test_by_keyword_list = __test_by_frequency_noun__(frequency_noun_list)\n",
    "    test_by_keyword_list= __test_by_keyword_list__(keyword_list)\n",
    "    test_limit_count = __test_limit_count__(lines, frequency_noun_list,keyword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
